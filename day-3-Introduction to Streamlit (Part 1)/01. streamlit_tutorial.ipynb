{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Tutorial: Building LLM Chat Applications 🚀\n",
    "\n",
    "This notebook provides a comprehensive guide to building interactive LLM applications using Streamlit. We'll start with basic concepts and build up to a complete chat application.\n",
    "\n",
    "## What is Streamlit?\n",
    "\n",
    "Streamlit is a Python library that makes it easy to create web applications for data science and machine learning. It's perfect for building interactive LLM applications because:\n",
    "- **Simple**: Write Python code, get a web app\n",
    "- **Interactive**: Real-time updates and user input\n",
    "- **Fast**: Hot-reloading during development\n",
    "- **Deployable**: Easy to share and deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit version: 1.45.1\n"
     ]
    }
   ],
   "source": [
    "# Install Streamlit if not already installed\n",
    "# !pip install streamlit\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Streamlit version:\", st.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Streamlit Elements Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Streamlit elements demonstrated above\n"
     ]
    }
   ],
   "source": [
    "# This cell demonstrates basic Streamlit elements\n",
    "# Note: In a real app, you'd put this code in a .py file and run with: streamlit run app.py\n",
    "\n",
    "def demonstrate_basic_elements():\n",
    "    \"\"\"\n",
    "    This function demonstrates basic Streamlit elements with explanations.\n",
    "    In a real app, you'd write this code directly in your .py file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. TITLE - Creates a main heading\n",
    "    # st.title() creates a large, prominent heading at the top of your app\n",
    "    st.title(\"🧠 LLM Chat Assistant\")\n",
    "    \n",
    "    # 2. TEXT INPUT - Creates an input field for user text\n",
    "    # st.text_input() creates a text box where users can type\n",
    "    # Parameters:\n",
    "    # - label: The text displayed above the input field\n",
    "    # - placeholder: Optional text shown when input is empty\n",
    "    # - key: Unique identifier for this widget (useful for session state)\n",
    "    prompt = st.text_input(\n",
    "        label=\"Enter your question or prompt:\",\n",
    "        placeholder=\"Type your message here...\",\n",
    "        key=\"user_prompt\"\n",
    "    )\n",
    "    \n",
    "    # 3. BUTTON - Creates a clickable button\n",
    "    # st.button() creates a button that returns True when clicked\n",
    "    # Parameters:\n",
    "    # - label: Text displayed on the button\n",
    "    # - key: Unique identifier\n",
    "    # - help: Tooltip text shown on hover\n",
    "    if st.button(\n",
    "        label=\"Generate Response\",\n",
    "        key=\"generate_btn\",\n",
    "        help=\"Click to generate LLM response\"\n",
    "    ):\n",
    "        # This code runs only when the button is clicked\n",
    "        st.write(\"Processing your prompt...\")\n",
    "    \n",
    "    # 4. MARKDOWN - Displays formatted text\n",
    "    # st.markdown() renders markdown text\n",
    "    # Supports: **bold**, *italic*, [links](url), headers (# ## ###), etc.\n",
    "    st.markdown(\"**LLM Response:** This will appear bold.\")\n",
    "    \n",
    "    # 5. SIDEBAR - Creates a sidebar for controls\n",
    "    # st.sidebar creates a sidebar on the left side of the app\n",
    "    # All widgets inside st.sidebar appear in the sidebar\n",
    "    with st.sidebar:\n",
    "        st.markdown(\"## Settings\")\n",
    "        \n",
    "        # 6. SELECTBOX - Creates a dropdown selection\n",
    "        # st.selectbox() creates a dropdown menu\n",
    "        # Parameters:\n",
    "        # - label: Text displayed above the dropdown\n",
    "        # - options: List of options to choose from\n",
    "        # - index: Default selected option (0-based)\n",
    "        model = st.selectbox(\n",
    "            label=\"Choose Model\",\n",
    "            options=[\"gpt-3.5-turbo\", \"gpt-4\", \"claude-3\", \"llama2\"],\n",
    "            index=0,\n",
    "            key=\"model_selector\"\n",
    "        )\n",
    "    \n",
    "    # 7. SESSION STATE - Persistent data across reruns\n",
    "    # st.session_state stores data that persists between interactions\n",
    "    # This is crucial for maintaining chat history and app state\n",
    "    if \"history\" not in st.session_state:\n",
    "        st.session_state[\"history\"] = []\n",
    "    \n",
    "    # Add to history when button is clicked\n",
    "    if st.button(\"Generate Response\") and prompt:\n",
    "        st.session_state[\"history\"].append({\n",
    "            \"user\": prompt,\n",
    "            \"assistant\": f\"Response to: {prompt}\",\n",
    "            \"timestamp\": datetime.now().strftime(\"%H:%M:%S\")\n",
    "        })\n",
    "    \n",
    "    # 8. STATUS MESSAGES - Different types of status displays\n",
    "    # st.success() - Green success message\n",
    "    st.success(\"LLM response generated successfully!\")\n",
    "    \n",
    "    # st.error() - Red error message\n",
    "    st.error(\"This is just a fake error for demonstration\")\n",
    "    \n",
    "    # st.warning() - Yellow warning message\n",
    "    st.warning(\"Please pay attention to the warning\")\n",
    "    \n",
    "    # st.info() - Blue info message\n",
    "    st.info(\"This is an informational message\")\n",
    "    \n",
    "    return prompt, model\n",
    "\n",
    "# Note: This function would be called in a real app\n",
    "# For demonstration purposes, we'll show the code structure\n",
    "print(\"Basic Streamlit elements demonstrated above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Streamlit Elements and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_advanced_elements():\n",
    "    \"\"\"\n",
    "    Demonstrates advanced Streamlit elements and best practices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. COLUMNS - Layout elements side by side\n",
    "    # st.columns() creates a multi-column layout\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    \n",
    "    with col1:\n",
    "        st.metric(\"Temperature\", \"0.7\", \"+0.1\")\n",
    "    with col2:\n",
    "        st.metric(\"Max Tokens\", \"150\", \"-10\")\n",
    "    with col3:\n",
    "        st.metric(\"Response Time\", \"2.3s\", \"-0.5s\")\n",
    "    \n",
    "    # 2. EXPANDER - Collapsible sections\n",
    "    # st.expander() creates a collapsible section\n",
    "    with st.expander(\"Advanced Settings\", expanded=False):\n",
    "        st.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n",
    "        st.slider(\"Max Tokens\", 50, 500, 150, 10)\n",
    "        st.checkbox(\"Enable streaming\", value=True)\n",
    "    \n",
    "    # 3. TABS - Organize content into tabs\n",
    "    # st.tabs() creates tabbed interface\n",
    "    tab1, tab2, tab3 = st.tabs([\"Chat\", \"History\", \"Settings\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.write(\"This is the chat tab\")\n",
    "        \n",
    "    with tab2:\n",
    "        st.write(\"This is the history tab\")\n",
    "        \n",
    "    with tab3:\n",
    "        st.write(\"This is the settings tab\")\n",
    "    \n",
    "    # 4. PROGRESS BAR - Show progress\n",
    "    # st.progress() creates a progress bar\n",
    "    progress_bar = st.progress(0)\n",
    "    for i in range(100):\n",
    "        # Simulate processing\n",
    "        import time\n",
    "        time.sleep(0.01)\n",
    "        progress_bar.progress(i + 1)\n",
    "    \n",
    "    # 5. SPINNER - Loading indicator\n",
    "    # st.spinner() shows a loading spinner\n",
    "    with st.spinner(\"Processing your request...\"):\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "    st.success(\"Done!\")\n",
    "    \n",
    "    # 6. FILE UPLOADER - Upload files\n",
    "    # st.file_uploader() allows file uploads\n",
    "    uploaded_file = st.file_uploader(\n",
    "        \"Choose a file\",\n",
    "        type=['txt', 'pdf', 'docx'],\n",
    "        help=\"Upload a document to analyze\"\n",
    "    )\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        st.write(f\"Uploaded: {uploaded_file.name}\")\n",
    "        \n",
    "    # 7. DOWNLOAD BUTTON - Download data\n",
    "    # st.download_button() creates a download button\n",
    "    csv_data = \"name,age\\nJohn,25\\nJane,30\"\n",
    "    st.download_button(\n",
    "        label=\"Download CSV\",\n",
    "        data=csv_data,\n",
    "        file_name=\"data.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "    \n",
    "    # 8. CONTAINER - Group elements\n",
    "    # st.container() groups elements together\n",
    "    with st.container():\n",
    "        st.write(\"This content is grouped together\")\n",
    "        st.button(\"Button inside container\")\n",
    "    \n",
    "    # 9. EMPTY - Placeholder for dynamic content\n",
    "    # st.empty() creates a placeholder for dynamic content\n",
    "    placeholder = st.empty()\n",
    "    if st.button(\"Update placeholder\"):\n",
    "        placeholder.write(\"This content was updated dynamically!\")\n",
    "    \n",
    "    # 10. FORM - Group form elements\n",
    "    # st.form() groups form elements and submits together\n",
    "    with st.form(\"my_form\"):\n",
    "        st.text_input(\"Name\")\n",
    "        st.text_input(\"Email\")\n",
    "        submitted = st.form_submit_button(\"Submit\")\n",
    "        \n",
    "        if submitted:\n",
    "            st.write(\"Form submitted!\")\n",
    "\n",
    "print(\"Advanced Streamlit elements demonstrated above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete LLM Chat Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete LLM Chat Application\n",
    "# This is the full application code that you can save as app.py\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"LLM Chat Assistant\",\n",
    "    page_icon=\"🧠\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for better styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".chat-message {\n",
    "    padding: 1rem;\n",
    "    border-radius: 0.5rem;\n",
    "    margin-bottom: 1rem;\n",
    "    display: flex;\n",
    "}\n",
    ".chat-message.user {\n",
    "    background-color: #2b313e;\n",
    "}\n",
    ".chat-message.assistant {\n",
    "    background-color: #475063;\n",
    "}\n",
    ".chat-message .avatar {\n",
    "    width: 20%;\n",
    "}\n",
    ".chat-message .message {\n",
    "    width: 80%;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"api_key\" not in st.session_state:\n",
    "    st.session_state.api_key = \"\"\n",
    "\n",
    "# Sidebar for settings\n",
    "with st.sidebar:\n",
    "    st.title(\"⚙️ Settings\")\n",
    "    \n",
    "    # Provider selection\n",
    "    provider = st.selectbox(\n",
    "        \"Choose Provider\",\n",
    "        [\"OpenAI\", \"Ollama\", \"Anthropic\"],\n",
    "        help=\"Select your LLM provider\"\n",
    "    )\n",
    "    \n",
    "    # Model selection based on provider\n",
    "    if provider == \"OpenAI\":\n",
    "        model = st.selectbox(\n",
    "            \"Choose Model\",\n",
    "            [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\"],\n",
    "            help=\"Select OpenAI model\"\n",
    "        )\n",
    "    elif provider == \"Ollama\":\n",
    "        model = st.selectbox(\n",
    "            \"Choose Model\",\n",
    "            [\"llama2\", \"codellama\", \"mistral\", \"neural-chat\"],\n",
    "            help=\"Select Ollama model\"\n",
    "        )\n",
    "    else:  # Anthropic\n",
    "        model = st.selectbox(\n",
    "            \"Choose Model\",\n",
    "            [\"claude-3-sonnet\", \"claude-3-opus\", \"claude-3-haiku\"],\n",
    "            help=\"Select Anthropic model\"\n",
    "        )\n",
    "    \n",
    "    # Model parameters\n",
    "    st.subheader(\"Model Parameters\")\n",
    "    temperature = st.slider(\n",
    "        \"Temperature\",\n",
    "        min_value=0.0,\n",
    "        max_value=2.0,\n",
    "        value=0.7,\n",
    "        step=0.1,\n",
    "        help=\"Controls randomness in responses\"\n",
    "    )\n",
    "    \n",
    "    max_tokens = st.slider(\n",
    "        \"Max Tokens\",\n",
    "        min_value=50,\n",
    "        max_value=2000,\n",
    "        value=500,\n",
    "        step=50,\n",
    "        help=\"Maximum length of response\"\n",
    "    )\n",
    "    \n",
    "    # API Configuration\n",
    "    st.subheader(\"API Configuration\")\n",
    "    \n",
    "    if provider == \"OpenAI\":\n",
    "        api_key = st.text_input(\n",
    "            \"OpenAI API Key\",\n",
    "            type=\"password\",\n",
    "            value=os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "            help=\"Enter your OpenAI API key\"\n",
    "        )\n",
    "    elif provider == \"Anthropic\":\n",
    "        api_key = st.text_input(\n",
    "            \"Anthropic API Key\",\n",
    "            type=\"password\",\n",
    "            value=os.getenv(\"ANTHROPIC_API_KEY\", \"\"),\n",
    "            help=\"Enter your Anthropic API key\"\n",
    "        )\n",
    "    else:  # Ollama\n",
    "        api_key = \"\"  # No API key needed for local Ollama\n",
    "        st.info(\"No API key needed for local Ollama\")\n",
    "    \n",
    "    # Chat controls\n",
    "    st.subheader(\"Chat Controls\")\n",
    "    \n",
    "    if st.button(\"🗑️ Clear Chat History\", type=\"secondary\"):\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "    \n",
    "    if st.button(\"📥 Export Chat\", type=\"secondary\"):\n",
    "        # Export chat history as JSON\n",
    "        chat_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"provider\": provider,\n",
    "            \"model\": model,\n",
    "            \"messages\": st.session_state.messages\n",
    "        }\n",
    "        \n",
    "        st.download_button(\n",
    "            label=\"Download Chat History\",\n",
    "            data=json.dumps(chat_data, indent=2),\n",
    "            file_name=f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
    "            mime=\"application/json\"\n",
    "        )\n",
    "\n",
    "# Main chat interface\n",
    "st.title(\"🧠 LLM Chat Assistant\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "        if \"timestamp\" in message:\n",
    "            st.caption(f\"{message['timestamp']}\")\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"What would you like to know?\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "        \"timestamp\": datetime.now().strftime(\"%H:%M:%S\")\n",
    "    })\n",
    "    \n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "        \n",
    "        try:\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                if provider == \"OpenAI\":\n",
    "                    # OpenAI API call\n",
    "                    import openai\n",
    "                    client = openai.OpenAI(api_key=api_key)\n",
    "                    \n",
    "                    response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=max_tokens,\n",
    "                        stream=True\n",
    "                    )\n",
    "                    \n",
    "                    for chunk in response:\n",
    "                        if chunk.choices[0].delta.content is not None:\n",
    "                            full_response += chunk.choices[0].delta.content\n",
    "                            message_placeholder.markdown(full_response + \"▌\")\n",
    "                    \n",
    "                    message_placeholder.markdown(full_response)\n",
    "                    \n",
    "                elif provider == \"Ollama\":\n",
    "                    # Ollama API call\n",
    "                    response = requests.post(\n",
    "                        \"http://localhost:11434/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": model,\n",
    "                            \"prompt\": prompt,\n",
    "                            \"stream\": True,\n",
    "                            \"options\": {\n",
    "                                \"temperature\": temperature,\n",
    "                                \"num_predict\": max_tokens\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    for line in response.iter_lines():\n",
    "                        if line:\n",
    "                            data = json.loads(line)\n",
    "                            if \"response\" in data:\n",
    "                                full_response += data[\"response\"]\n",
    "                                message_placeholder.markdown(full_response + \"▌\")\n",
    "                    \n",
    "                    message_placeholder.markdown(full_response)\n",
    "                    \n",
    "                else:  # Anthropic\n",
    "                    # Anthropic API call\n",
    "                    import anthropic\n",
    "                    client = anthropic.Anthropic(api_key=api_key)\n",
    "                    \n",
    "                    response = client.messages.create(\n",
    "                        model=model,\n",
    "                        max_tokens=max_tokens,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    \n",
    "                    full_response = response.content[0].text\n",
    "                    message_placeholder.markdown(full_response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {str(e)}\")\n",
    "            full_response = f\"Sorry, I encountered an error: {str(e)}\"\n",
    "            message_placeholder.markdown(full_response)\n",
    "        \n",
    "        # Add assistant response to chat history\n",
    "        st.session_state.messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response,\n",
    "            \"timestamp\": datetime.now().strftime(\"%H:%M:%S\")\n",
    "        })\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    f\"<div style='text-align: center; color: #666;'>\"\n",
    "    f\"Using {provider} with {model} • \"\n",
    "    f\"Messages: {len(st.session_state.messages)} • \"\n",
    "    f\"Temperature: {temperature} • \"\n",
    "    f\"Max Tokens: {max_tokens}\"\n",
    "    f\"</div>\",\n",
    "    unsafe_allow_html=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Important Streamlit Commands Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Streamlit Commands Reference\n",
    "\n",
    "def streamlit_commands_reference():\n",
    "    \"\"\"\n",
    "    Comprehensive reference of important Streamlit commands\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\"\"\n",
    "=== STREAMLIT COMMANDS REFERENCE ===\n",
    "\n",
    "📝 TEXT AND DISPLAY:\n",
    "st.write()           - Display any data type\n",
    "st.text()            - Display plain text\n",
    "st.markdown()        - Display markdown text\n",
    "st.latex()           - Display LaTeX math\n",
    "st.code()            - Display code with syntax highlighting\n",
    "st.json()            - Display JSON data\n",
    "\n",
    "📊 DATA DISPLAY:\n",
    "st.dataframe()       - Display pandas DataFrame\n",
    "st.table()           - Display static table\n",
    "st.metric()          - Display key metrics\n",
    "st.line_chart()      - Line chart\n",
    "st.bar_chart()       - Bar chart\n",
    "st.area_chart()      - Area chart\n",
    "st.scatter_chart()   - Scatter plot\n",
    "st.map()             - Display map\n",
    "st.plotly_chart()    - Display Plotly charts\n",
    "st.altair_chart()    - Display Altair charts\n",
    "\n",
    "🎛️ INPUT WIDGETS:\n",
    "st.text_input()      - Single line text input\n",
    "st.text_area()       - Multi-line text input\n",
    "st.number_input()    - Number input\n",
    "st.selectbox()       - Dropdown selection\n",
    "st.multiselect()     - Multi-selection dropdown\n",
    "st.slider()          - Slider input\n",
    "st.select_slider()   - Select from range\n",
    "st.checkbox()        - Checkbox\n",
    "st.radio()           - Radio buttons\n",
    "st.button()          - Button\n",
    "st.download_button() - Download button\n",
    "st.file_uploader()   - File upload\n",
    "st.camera_input()    - Camera input\n",
    "st.color_picker()    - Color picker\n",
    "st.date_input()      - Date input\n",
    "st.time_input()      - Time input\n",
    "\n",
    "📱 LAYOUT:\n",
    "st.sidebar           - Sidebar container\n",
    "st.columns()         - Multi-column layout\n",
    "st.container()       - Container for grouping\n",
    "st.expander()        - Collapsible section\n",
    "st.tabs()            - Tabbed interface\n",
    "st.empty()           - Placeholder for dynamic content\n",
    "\n",
    "💬 CHAT INTERFACE:\n",
    "st.chat_input()      - Chat input field\n",
    "st.chat_message()    - Chat message container\n",
    "\n",
    "📊 CHARTS AND PLOTS:\n",
    "st.line_chart()      - Line chart\n",
    "st.bar_chart()       - Bar chart\n",
    "st.area_chart()      - Area chart\n",
    "st.scatter_chart()   - Scatter plot\n",
    "st.map()             - Map display\n",
    "st.plotly_chart()    - Plotly integration\n",
    "st.altair_chart()    - Altair integration\n",
    "st.vega_lite_chart() - Vega-Lite charts\n",
    "\n",
    "⚙️ CONFIGURATION:\n",
    "st.set_page_config() - Page configuration\n",
    "st.experimental_memo - Cache function results\n",
    "st.cache_data        - Cache data loading\n",
    "st.cache_resource    - Cache resources\n",
    "\n",
    "🔄 STATE MANAGEMENT:\n",
    "st.session_state     - Persistent state across reruns\n",
    "st.rerun()           - Rerun the app\n",
    "st.stop()            - Stop execution\n",
    "\n",
    "📱 MOBILE AND RESPONSIVE:\n",
    "st.set_page_config(layout=\"wide\")  - Wide layout\n",
    "st.set_page_config(layout=\"centered\")  - Centered layout\n",
    "\n",
    "🎨 STYLING:\n",
    "st.markdown() with HTML - Custom styling\n",
    "st.components.html() - Custom HTML components\n",
    "st.components.iframe() - Embed external content\n",
    "\n",
    "📊 PROGRESS AND STATUS:\n",
    "st.progress()        - Progress bar\n",
    "st.spinner()         - Loading spinner\n",
    "st.balloons()        - Celebration balloons\n",
    "st.snow()            - Snow effect\n",
    "\n",
    "💬 MESSAGES:\n",
    "st.success()         - Success message\n",
    "st.error()           - Error message\n",
    "st.warning()         - Warning message\n",
    "st.info()            - Info message\n",
    "st.exception()       - Display exception\n",
    "\n",
    "📁 FILE HANDLING:\n",
    "st.file_uploader()   - Upload files\n",
    "st.download_button() - Download files\n",
    "\n",
    "🔧 FORMS:\n",
    "st.form()            - Form container\n",
    "st.form_submit_button() - Form submit button\n",
    "\n",
    "📊 CACHING:\n",
    "@st.cache_data       - Cache data loading functions\n",
    "@st.cache_resource   - Cache resource loading\n",
    "@st.experimental_memo - Cache function results\n",
    "\n",
    "🎯 BEST PRACTICES:\n",
    "- Use st.session_state for persistent data\n",
    "- Cache expensive operations with @st.cache_data\n",
    "- Use st.empty() for dynamic content updates\n",
    "- Group related widgets in containers\n",
    "- Use st.sidebar for settings and controls\n",
    "- Implement proper error handling\n",
    "- Use st.spinner() for long operations\n",
    "- Optimize for mobile with responsive layouts\n",
    "\"\"\")\n",
    "\n",
    "streamlit_commands_reference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Your Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to run your Streamlit app\n",
    "\n",
    "print(\"\"\"\n",
    "=== RUNNING YOUR STREAMLIT APP ===\n",
    "\n",
    "1. Save your code as app.py\n",
    "2. Open terminal/command prompt\n",
    "3. Navigate to your project directory\n",
    "4. Run: streamlit run app.py\n",
    "5. Open browser to http://localhost:8501\n",
    "\n",
    "Additional commands:\n",
    "- streamlit run app.py --server.port 8502  # Custom port\n",
    "- streamlit run app.py --server.address 0.0.0.0  # Allow external access\n",
    "- streamlit run app.py --server.headless true  # Headless mode\n",
    "- streamlit run app.py --browser.gatherUsageStats false  # Disable usage stats\n",
    "\n",
    "Development tips:\n",
    "- Use --server.runOnSave true for auto-reload\n",
    "- Use --server.enableCORS false for local development\n",
    "- Use --server.enableXsrfProtection false for testing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment options for Streamlit apps\n",
    "\n",
    "print(\"\"\"\n",
    "=== DEPLOYMENT OPTIONS ===\n",
    "\n",
    "🌐 Streamlit Cloud (Recommended):\n",
    "- Free hosting for public repos\n",
    "- Automatic deployment from GitHub\n",
    "- Easy sharing and collaboration\n",
    "- Built-in CI/CD\n",
    "\n",
    "🐳 Docker:\n",
    "- Containerized deployment\n",
    "- Consistent environment\n",
    "- Easy scaling\n",
    "- Cloud platform support\n",
    "\n",
    "☁️ Cloud Platforms:\n",
    "- Heroku\n",
    "- Google Cloud Run\n",
    "- AWS App Runner\n",
    "- Azure Container Instances\n",
    "\n",
    "🖥️ Self-hosted:\n",
    "- VPS deployment\n",
    "- Custom domain\n",
    "- Full control\n",
    "- Cost-effective for high traffic\n",
    "\n",
    "📱 Mobile:\n",
    "- Streamlit apps work on mobile\n",
    "- Responsive design recommended\n",
    "- Touch-friendly interfaces\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "✅ **Basic Streamlit Elements**: title, text_input, button, markdown, sidebar, session_state\n",
    "✅ **Advanced Features**: columns, expanders, tabs, progress bars, spinners, file uploads\n",
    "✅ **Complete Chat Application**: Full-featured LLM chat with multiple providers\n",
    "✅ **Important Commands**: Comprehensive reference of Streamlit commands\n",
    "✅ **Best Practices**: State management, caching, error handling, responsive design\n",
    "✅ **Deployment**: Various options for hosting your Streamlit apps\n",
    "\n",
    "**Next Steps**:\n",
    "1. Save the complete app code as `app.py`\n",
    "2. Run with `streamlit run app.py`\n",
    "3. Customize for your specific use case\n",
    "4. Deploy to Streamlit Cloud or your preferred platform\n",
    "\n",
    "Happy coding! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
